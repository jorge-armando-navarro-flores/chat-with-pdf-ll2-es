What Is a Vector Database?
And why you should put your embeddings in one
Rupert Thomas
Rupert Thomas

·
Follow

4 min read
·
Jun 27
1






tldr: Vector databases are optimized for the storage and retrieval of embedding vectors, as generated by AI models. Embedding vectors are mathematical representations of data that capture its essential characteristics and semantic relationships. Unlike traditional databases that excel at finding data based on an exact match, vector databases can efficiently execute similarity searches. They are used in applications such as content search, recommender systems, anomaly and fraud detection.

In this article we will cover:

what is an embedding vector,
how to use vector operations to find similar content,
why a vector database is preferable for storing embedding vectors, and
examples of commonly used vector databases.
Embedding vectors
An embedding vector is a mathematical representation of data that preserves its essential characteristics and semantic relationships. AI models (such as Transformers) generate embedding vectors as intermediate representations, but they are valuable in their own right as a tool for working with data. Images, words, audio and other data types can all be represented in vector form.

For example, in natural language processing (NLP), a sentence or word can be transformed into a vector that encapsulates semantic information such as meaning, context, and syntactic relationships. Similarly, in image processing, an embedding vector can capture visual characteristics, allowing for image similarity comparisons or content-based image retrieval.

Embedding vectors can be visualized as encoding a point (or vector) in multi-dimensional space, whereby each number describes the position on a different axis.


A vector of length 3 encodes a position in a 3-dimensional space. Language models typically encode to more complicated spaces; BERT produces a vector that has length 768. (Image credit: author)
Crucially, the embedding vectors from similar content (such as images, words etc) end up positioned close to each other in the vector space. This property can be exploited to search for similar content without being restricted to exact matches, or to understand when two pieces of data relate to the same topic. For example, if a user searches on a shopping website for “winter boots” we would like to return results such as “fur-lined boots” first, even though the word “winter” is not in the item description.

To do this, we would process the descriptions of all the items in the shop using an Encoder (a type of AI model), to generate embedding vectors for each. Then when a user enters a search query, we would also generate an embedding vector for the search query and find which items in our shop are most like the query vector.

Vector operations for similarity searching
To find similar content based on a search query, we need to compare vectors and find ones that are located nearby. We could just measure the straight-line distance between vectors (the Euclidean distance), although when dealing with very high-dimensional spaces it is more common to measure the angle between the vectors. This is referred to as the cosine similarity.

Cosine similarity measures the cosine of the angle between two vectors, treating them as vectors originating from the origin of a coordinate system. It disregards the magnitude of the vectors and focuses on the relative orientation or direction. The formula for cosine similarity between vectors A and B is:

cosine similarity = (A · B) / (||A|| * ||B||)

Here, (A · B) is the dot (inner) product of vectors A and B, and ||A|| and ||B|| are the magnitudes of the vectors.

Cosine similarity ranges from -1 to 1, where 1 show perfect similarity, 0 indicates no similarity, and -1 indicates perfect dissimilarity or opposite directions. Unlike Euclidean distance, cosine similarity is not affected by the scale or magnitude of the vectors, making it useful for comparing the similarity of vectors irrespective of their lengths.

Vector databases
While it would be possible to request the embedding vector for every product on the shopping website and then compute the similarity between each one and the search query embedding, this scales poorly when there are many thousands of products. A better approach is to move this computation towards where the data is stored, by using a database specifically designed for this purpose.

Vector databases are preferred over traditional databases when dealing with embedding vectors or high-dimensional data because:

Efficient similarity search: vector databases are specifically designed to perform similarity search operations. Their index is structured to enable fast retrieval of nearest neighbours or similar vectors.
High-dimensional indexing: Traditional databases often struggle with indexing and querying high-dimensional data efficiently. Vector databases use specialized indexing techniques for high-dimensional spaces — such as projecting into a lower dimensional space (e.g. using random projections) .
Vector databases excel at executing similarity searches over large datasets — something that would be very slow (or impossible) with traditional databases. With a vector representation of any search query (e.g. a text string, an image, or other data source that can be embedded), we can find content that is most similar. This is useful for a wide range of applications, such as:

E-commerce (matching products to users)
Recommendation systems (e.g. Netflix — finding similar content)
Market segmentation (identifying clusters of similar data)
Drug discovery (finding similar protein interactions)
Popular vector databases
Handling embedding vectors is a fundamental component of AI based systems, so a variety of vector databases have now been developed, including:

Milvus
Weaviate
Pinecone
Elasticsearch — now supports vector field types
Faiss — technically a library rather than a database
When deciding on a vector database solution it is worth considering scalability and performance, ease of integration, the size of the market ecosystem, data persistence and durability, administration, security and compliance.

